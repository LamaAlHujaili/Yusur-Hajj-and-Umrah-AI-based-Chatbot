{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai langchain tiktoken"
      ],
      "metadata": {
        "id": "xdeNAZ0mKLsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIdCjMvtc5YB",
        "outputId": "41b94a07-ed4c-4521-e531-02c3df22983c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Imports\n",
        "import google.generativeai as genai\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import openai"
      ],
      "metadata": {
        "id": "sSFa5IFUKOOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Gemini API\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n"
      ],
      "metadata": {
        "id": "yV5gWlbsKRoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup GPT04 API\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "wC3xLjpDYThL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload documents\n",
        "uploaded = files.upload()\n",
        "eng_text = open(\"/content/extractedEng_text.txt\", encoding=\"utf-8\").read()\n",
        "ara_text = open(\"/content/extractedAra_text.txt\", encoding=\"utf-8\").read()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Lh7oDWUjKTy1",
        "outputId": "648de546-da1a-44b4-ffa4-8e2216f0af21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-56839ac4-5f90-4f08-9735-bb66ef1bdc23\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-56839ac4-5f90-4f08-9735-bb66ef1bdc23\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving extractedAra_text.txt to extractedAra_text (1).txt\n",
            "Saving extractedEng_text.txt to extractedEng_text (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk documents\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=75)\n",
        "english_chunks = splitter.split_text(eng_text)\n",
        "arabic_chunks = splitter.split_text(ara_text)"
      ],
      "metadata": {
        "id": "ZFS1KDUxXyRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prompt_en(context):\n",
        "    return f\"\"\"\n",
        "You are a helpful assistant generating test questions for a retrieval-based system used by pilgrims during Hajj and Umrah.\n",
        "\n",
        "Generate 4 types of questions from the text:\n",
        "1. Conditional (e.g., \"If a pilgrim forgets to do X, what should they do?\")\n",
        "2. Reasoning (e.g., \"Why is it important to do X during Y?\")\n",
        "3. Scenario (e.g., \"I am performing Umrah and does X. What should I do next?\")\n",
        "4. Simple factual (e.g., \"How many rounds are in Tawaf?\")\n",
        "\n",
        "Make sure:\n",
        "- All questions can be answered strictly based on the text.\n",
        "- Avoid questions that refer to the document itself like what is the purpose of this document or where can i find this information in the document or general advice not found in the text.\n",
        "-Do not generate questions out of the documents and Do not generate questions there answers not in the documents\n",
        "\n",
        "Return in JSON format like this:\n",
        "{{\n",
        "  \"questions\": [\n",
        "    {{\"type\": \"conditional\", \"question\": \"...\", \"answer\": \"...\"}},\n",
        "    {{\"type\": \"reasoning\", \"question\": \"...\", \"answer\": \"...\"}},\n",
        "    {{\"type\": \"scenario\", \"question\": \"...\", \"answer\": \"...\"}},\n",
        "    {{\"type\": \"simple\", \"question\": \"...\", \"answer\": \"...\"}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "def prompt_ar(context):\n",
        "    return f\"\"\"\n",
        "أنت مساعد ذكي يساعد في توليد أنواع مختلفة من الأسئلة من النصوص الخاصة بالحج والعمرة.\n",
        "\n",
        "أنشئ ٤ أنواع من الأسئلة بناءً على النص:\n",
        "1. شرطية (مثال: \"إذا نسي الحاج أن يفعل كذا، ماذا يجب أن يفعل؟\")\n",
        "2. استنتاجية (مثال: \"لماذا يعتبر فعل كذا مهماً أثناء كذا؟\")\n",
        "3. سيناريو (مثال: \"أنا أؤدي العمرة وقمت بـفعل كذا . ماذا يجب أن أفعل بعد ذلك؟\")\n",
        "4. بسيطة مباشرة (مثال: \"كم عدد أشواط الطواف؟\")\n",
        "\n",
        "الشروط:\n",
        "يمكن الإجابة على جميع الأسئلة اعتمادًا فقط على النص.\n",
        "\n",
        "تجنّب الأسئلة التي تشير إلى الوثيقة نفسها مثل: ما الهدف من هذه الوثيقة؟ أو أين يمكنني العثور على هذه المعلومات في الوثيقة؟ أو نصائح عامة غير موجودة في النص.\n",
        "\n",
        "لا تُنشئ أسئلة من خارج الوثيقة، ولا تُنشئ أسئلة لا توجد إجاباتها داخل الوثيقة.\n",
        "\n",
        "\n",
        "\n",
        "أعد النتيجة بصيغة JSON كالتالي:\n",
        "{{\n",
        "  \"questions\": [\n",
        "    {{\"type\": \"conditional\", \"question\": \"...\", \"answer\": \"...\"}},\n",
        "    {{\"type\": \"reasoning\", \"question\": \"...\", \"answer\": \"...\"}},\n",
        "    {{\"type\": \"scenario\", \"question\": \"...\", \"answer\": \"...\"}},\n",
        "    {{\"type\": \"simple\", \"question\": \"...\", \"answer\": \"...\"}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "النص:\n",
        "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ufv3q9rwUGKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Helper: clean Gemini markdown output\n",
        "def extract_json(text):\n",
        "    match = re.search(r\"```json(.*?)```\", text, re.DOTALL)\n",
        "    return match.group(1).strip() if match else text.strip()\n",
        "\n",
        "# Helper: filter weak QAs\n",
        "def is_valid(qa):\n",
        "    q, a = qa.get(\"question\", \"\").strip(), qa.get(\"answer\", \"\").strip()\n",
        "    return (\n",
        "        len(q) > 8 and \"not available in the text\" not in a.lower() and\n",
        "        \"check the guide\" not in a.lower() and\n",
        "        len(a) > 10\n",
        "    )\n"
      ],
      "metadata": {
        "id": "seKjmeGOKdU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Gemeni Generarion\n",
        "results = []\n",
        "def generate_qa(chunks, lang_code, prompt_fn, start=0, limit=20):\n",
        "    for i, chunk in enumerate(chunks[start:start + limit]):\n",
        "        prompt = prompt_fn(chunk)\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            cleaned = extract_json(response.text)\n",
        "            parsed = json.loads(cleaned)\n",
        "            for qa in parsed.get(\"questions\", []):\n",
        "                if is_valid(qa):\n",
        "                    results.append({\n",
        "                        \"question\": qa[\"question\"],\n",
        "                        \"answer\": qa[\"answer\"],\n",
        "                        \"context\": chunk,\n",
        "                        \"language\": \"arabic\" if lang_code == \"ar\" else \"english\",\n",
        "                        \"type\": qa.get(\"type\", \"unknown\"),\n",
        "                        \"model\": \"gemini-1.5-flash\"\n",
        "                    })\n",
        "            print(f\"✅ Valid QAs added from {lang_code} chunk {i+1}/{limit}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error ({lang_code} chunk {i+1}):\", e)\n",
        "        time.sleep(2)  # avoid quota spikes"
      ],
      "metadata": {
        "id": "fui-J4EVKhSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT Generation\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "# results = []\n",
        "\n",
        "# def generate_qa(chunks, lang_code, prompt_fn, limit=50):\n",
        "#     for i, chunk in enumerate(chunks[:limit]):\n",
        "#         prompt = prompt_fn(chunk)\n",
        "#         try:\n",
        "#             response = openai.ChatCompletion.create(\n",
        "#                 model=\"gpt-4\",\n",
        "#                 messages=[\n",
        "#                     {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates question-answer pairs in JSON format.\"},\n",
        "#                     {\"role\": \"user\", \"content\": prompt}\n",
        "#                 ]\n",
        "#             )\n",
        "#             response_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "#             cleaned = extract_json(response_text)\n",
        "#             parsed = json.loads(cleaned)\n",
        "\n",
        "#             for qa in parsed.get(\"questions\", []):\n",
        "#                 if is_valid(qa):\n",
        "#                     results.append({\n",
        "#                         \"question\": qa[\"question\"],\n",
        "#                         \"answer\": qa[\"answer\"],\n",
        "#                         \"context\": chunk,\n",
        "#                         \"language\": \"arabic\" if lang_code == \"ar\" else \"english\",\n",
        "#                         \"type\": qa.get(\"type\", \"unknown\"),\n",
        "#                         \"model\": \"gpt-4\"\n",
        "#                     })\n",
        "#             print(f\"✅ Valid QAs added from {lang_code} chunk {i+1}/{limit}\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"❌ Error ({lang_code} chunk {i+1}):\", e)\n",
        "\n",
        "#         time.sleep(2)\n"
      ],
      "metadata": {
        "id": "sipS6gkca3Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run both languages\n",
        "\n",
        "#GPT\n",
        "# generate_qa(english_chunks, \"en\", prompt_en, limit=20) #first 20 chunks\n",
        "# generate_qa(arabic_chunks, \"ar\", prompt_ar, limit=20) #first 20 chunks\n",
        "\n",
        "\n",
        "#Gemeni\n",
        "generate_qa(english_chunks, \"en\", prompt_en,start=20, limit=20) # chunks 20–39\n",
        "generate_qa(arabic_chunks, \"ar\", prompt_ar,start=20, limit=20) # chunks 20–39\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "av9xwehEKkNM",
        "outputId": "9eb5220c-a137-487a-e845-edf498a91dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Valid QAs added from en chunk 1/20\n",
            "✅ Valid QAs added from en chunk 2/20\n",
            "✅ Valid QAs added from en chunk 3/20\n",
            "✅ Valid QAs added from en chunk 4/20\n",
            "✅ Valid QAs added from en chunk 5/20\n",
            "✅ Valid QAs added from en chunk 6/20\n",
            "✅ Valid QAs added from en chunk 7/20\n",
            "✅ Valid QAs added from en chunk 8/20\n",
            "✅ Valid QAs added from en chunk 9/20\n",
            "✅ Valid QAs added from en chunk 10/20\n",
            "✅ Valid QAs added from en chunk 11/20\n",
            "✅ Valid QAs added from en chunk 12/20\n",
            "✅ Valid QAs added from en chunk 13/20\n",
            "✅ Valid QAs added from en chunk 14/20\n",
            "✅ Valid QAs added from en chunk 15/20\n",
            "✅ Valid QAs added from en chunk 16/20\n",
            "✅ Valid QAs added from en chunk 17/20\n",
            "✅ Valid QAs added from en chunk 18/20\n",
            "✅ Valid QAs added from en chunk 19/20\n",
            "✅ Valid QAs added from en chunk 20/20\n",
            "✅ Valid QAs added from ar chunk 1/20\n",
            "✅ Valid QAs added from ar chunk 2/20\n",
            "✅ Valid QAs added from ar chunk 3/20\n",
            "✅ Valid QAs added from ar chunk 4/20\n",
            "✅ Valid QAs added from ar chunk 5/20\n",
            "✅ Valid QAs added from ar chunk 6/20\n",
            "✅ Valid QAs added from ar chunk 7/20\n",
            "✅ Valid QAs added from ar chunk 8/20\n",
            "✅ Valid QAs added from ar chunk 9/20\n",
            "✅ Valid QAs added from ar chunk 10/20\n",
            "✅ Valid QAs added from ar chunk 11/20\n",
            "✅ Valid QAs added from ar chunk 12/20\n",
            "✅ Valid QAs added from ar chunk 13/20\n",
            "✅ Valid QAs added from ar chunk 14/20\n",
            "✅ Valid QAs added from ar chunk 15/20\n",
            "✅ Valid QAs added from ar chunk 16/20\n",
            "✅ Valid QAs added from ar chunk 17/20\n",
            "✅ Valid QAs added from ar chunk 18/20\n",
            "✅ Valid QAs added from ar chunk 19/20\n",
            "✅ Valid QAs added from ar chunk 20/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the output using only JSON format\n",
        "import json\n",
        "\n",
        "with open(\"TestsetGemeni2.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "files.download(\"TestsetGemeni2.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "djIo72IPOV43",
        "outputId": "1dde6249-3c09-48ba-8ee9-d3cf9d0e6d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4354d91e-b57b-47c8-a934-6e7c8afd9ace\", \"TestsetGemeni2.json\", 257130)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}