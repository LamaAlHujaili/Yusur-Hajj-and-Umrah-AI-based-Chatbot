# -*- coding: utf-8 -*-
"""Full_Implementation_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y8U-7eVb1pZmIeMUu2_j7EDgtImO2erm
"""

# STEP 1: Install Required Libraries
!pip install transformers datasets faiss-cpu sentence-transformers
!pip install langchain-openai langchain langchain-community chromadb pypdf tiktoken
!pip install langchain_experimental langchain_huggingface
!pip install google-generativeai langchain-google-genai
!pip install accelerate torch torchvision torchaudio
!pip install bitsandbytes

# STEP 2: Import Libraries
import os
import re
import torch
import faiss
import openai
import numpy as np
import google.generativeai as genai
from transformers import AutoTokenizer, AutoModel, pipeline
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from google.colab import files
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnablePassthrough
from langchain.vectorstores import Chroma
from langchain.schema import Document
from langchain.retrievers import ContextualCompressionRetriever

# STEP 3: Upload and Load Documents
uploaded = files.upload()
docs = []
for filename in uploaded.keys():
    with open(filename, encoding='utf-8') as f:
        docs.append(f.read())

# STEP 4: Chunk the Documents
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
documents = [Document(page_content=chunk) for doc in docs for chunk in text_splitter.split_text(doc)]

# STEP 5: Embedding + Vector Store (Chroma)
from langchain.embeddings import HuggingFaceEmbeddings
import os

# Use E5 multilingual model (high quality for both Arabic and English)
embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")

# Load or create vectorstore
if os.path.exists("chroma_db_e5"):
    vectorstore = Chroma(persist_directory="chroma_db_e5", embedding_function=embedding_model)
else:
    vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory="chroma_db_e5")
    vectorstore.persist()

retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# STEP 6: Apply AraBERT Reranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker

reranker = HuggingFaceCrossEncoder(model_name="Omartificial-Intelligence-Space/ARA-Reranker-V1")
compressor = CrossEncoderReranker(model=reranker, top_n=10)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)

# STEP 7: Prompt Formatter
def detect_language(text):
    arabic_chars = re.findall(r'[\u0600-\u06FF]', text)
    return "ar" if len(arabic_chars) / len(text) > 0.3 else "en"

def build_prompt(question, context_chunks, lang="en"):
    context = "\n".join(context_chunks)

    if lang == "ar":
        return f"""
        أنت مساعد ذكي ومؤتمن مخصص لخدمة الحجاج والمعتمرين، مهمتك هي تقديم إجابات دقيقة، وافية، ومفهومة لجميع أسئلتهم المتعلقة بالحج والعمرة، بما في ذلك المسائل الدينية، التنظيمية، الصحية، والعملية. يجب أن تكون إجاباتك شاملة، تراعي اختلاف مستويات فهم المستخدمين، وتتناسب مع احتياجاتهم أثناء أداء المناسك.

أجب فقط باستخدام المعلومات المتوفرة في السياق المعطى، ولا تعتمد على الحدس أو المعرفة العامة خارج هذا السياق. اقرأ وتحلّل جميع أجزاء السياق بدقة قبل الإجابة، ولا تكتفِ بأول جزء يحتوي على كلمات مشابهة للسؤال. وظّف جميع التفاصيل المتاحة لتقديم إجابة دقيقة وملائمة.

- إذا ورد في السؤال كلمة "دليل"، فالمقصود بها دليل شرعي من القرآن الكريم أو السنة النبوية الشريفة، وليس دليلاً تنظيمياً أو من وثائق عامة.
- إذا لم تجد إجابة واضحة في السياق، فأجب بعبارة: "الإجابة غير موجودة ."
- لا تُدخل أي معلومات غير موجودة في السياق حتى وإن كنت تعتقد بصحتها.
- افترض أن المستخدم قد يكون غير ملم بالمصطلحات الشرعية أو التنظيمية، لذلك احرص على الشرح بلغة مبسطة وواضحة قدر الإمكان.
- كن صبورًا ومراعياً، فالمستخدم قد يكون في حالة تعب أو ارتباك أثناء أداء المناسك، وسؤاله قد لا يكون دقيقاً، لذا حاول فهم مقصده الحقيقي قبل الرد.
- الهدف هو أن يشعر المستخدم بأنك مساعد موثوق، يفهمه جيدًا، ويوفر له الدعم اللازم لأداء مناسكه بسهولة ويسر.
- لا تذكر السياق أو تشير إليه في إجابتك، أجب وكأنك تملك المعرفة بشكل مباشر.
- إذا سأل المستخدم عن فعل معين، فلا تكتفِ بذكر اسم الفعل أو عنوانه فقط، بل ارشده خطوة بخطوة، واذكر النصوص الكاملة إذا كانت متوفرة، مثل الأدعية أو الأقوال.

السياق:
{context}

السؤال: {question}
الإجابة:"""

    else:
        return f"""You are an intelligent and trustworthy assistant designed to support pilgrims during Hajj and Umrah. Your role is to provide accurate, comprehensive, and easy-to-understand answers to all user questions related to the pilgrimage, including religious, procedural, health-related, and logistical matters. Your responses must be clear, suitable for users from all backgrounds, and tailored to their needs during the performance of rituals.

Only answer using the information provided in the given context. Do not rely on general knowledge or assumptions. Carefully read and analyze the entire context before answering — do not rely solely on the first paragraph or the presence of keywords. Use all available details to deliver a precise and contextually relevant answer.

- If the user mentions the word "dalil" or "proof", they are referring to religious evidence from the Qur’an or the Sunnah, not from general documents or guides.
- If the answer cannot be found in the context, respond with: "The answer is not available in the provided context."
- Never provide any information that is not explicitly present in the context.
- Assume that the user may not be familiar with religious or technical terms — always explain clearly and simply when necessary.
- Be patient and empathetic — the user might be tired, confused, or under stress during their pilgrimage. Try to understand their true intent and respond accordingly.
- Your ultimate goal is to make the user feel supported, understood, and confident in performing their pilgrimage properly and with ease.
- Do not mention or refer to the context in your answer; respond as if you know the information directly.
- If the user asks about a specific action, do not just mention the name or title of the action — guide them step by step and include the full phrases or sayings if available, such as supplications or rituals.

Context:
{context}

Question: {question}
Answer:"""

# Output cleaner
def clean_output(output):
    if isinstance(output, dict) and 'content' in output:
        return output['content'].strip()
    elif hasattr(output, 'content'):
        return output.content.strip()
    elif isinstance(output, str):
        return output.strip()
    return str(output)

# STEP 8: Gemini
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

#Add your API Key here

llm_gemini = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.7, max_output_tokens=256)


gemini_prompt_ar = ChatPromptTemplate.from_template("""
<|system|>
أنت مساعد ذكي ومؤتمن مخصص لخدمة الحجاج والمعتمرين، مهمتك هي تقديم إجابات دقيقة، وافية، ومفهومة لجميع أسئلتهم المتعلقة بالحج والعمرة، بما في ذلك المسائل الدينية، التنظيمية، الصحية، والعملية. يجب أن تكون إجاباتك شاملة، تراعي اختلاف مستويات فهم المستخدمين، وتتناسب مع احتياجاتهم أثناء أداء المناسك.

أجب فقط باستخدام المعلومات المتوفرة في السياق المعطى، ولا تعتمد على الحدس أو المعرفة العامة خارج هذا السياق. اقرأ وتحلّل جميع أجزاء السياق بدقة قبل الإجابة، ولا تكتفِ بأول جزء يحتوي على كلمات مشابهة للسؤال. وظّف جميع التفاصيل المتاحة لتقديم إجابة دقيقة وملائمة.

- إذا ورد في السؤال كلمة "دليل"، فالمقصود بها دليل شرعي من القرآن الكريم أو السنة النبوية الشريفة، وليس دليلاً تنظيمياً أو من وثائق عامة.
- إذا لم تجد إجابة واضحة في السياق، فأجب بعبارة: "الإجابة غير موجودة ."
- لا تُدخل أي معلومات غير موجودة في السياق حتى وإن كنت تعتقد بصحتها.
- افترض أن المستخدم قد يكون غير ملم بالمصطلحات الشرعية أو التنظيمية، لذلك احرص على الشرح بلغة مبسطة وواضحة قدر الإمكان.
- كن صبورًا ومراعياً، فالمستخدم قد يكون في حالة تعب أو ارتباك أثناء أداء المناسك، وسؤاله قد لا يكون دقيقاً، لذا حاول فهم مقصده الحقيقي قبل الرد.
- الهدف هو أن يشعر المستخدم بأنك مساعد موثوق، يفهمه جيدًا، ويوفر له الدعم اللازم لأداء مناسكه بسهولة ويسر.
- لا تذكر السياق أو تشير إليه في إجابتك، أجب وكأنك تملك المعرفة بشكل مباشر.
- إذا سأل المستخدم عن فعل معين، فلا تكتفِ بذكر اسم الفعل أو عنوانه فقط، بل ارشده خطوة بخطوة، واذكر النصوص الكاملة إذا كانت متوفرة، مثل الأدعية أو الأقوال.

</s>
<|user|>
السياق:
{context}

السؤال:
{query}
</s>
<|assistant|>
""")


gemini_prompt_en = ChatPromptTemplate.from_template("""
<|system|>
You are an intelligent and trustworthy assistant designed to support pilgrims during Hajj and Umrah. Your role is to provide accurate, comprehensive, and easy-to-understand answers to all user questions related to the pilgrimage, including religious, procedural, health-related, and logistical matters. Your responses must be clear, suitable for users from all backgrounds, and tailored to their needs during the performance of rituals.

Only answer using the information provided in the given context. Do not rely on general knowledge or assumptions. Carefully read and analyze the entire context before answering — do not rely solely on the first paragraph or the presence of keywords. Use all available details to deliver a precise and contextually relevant answer.

- If the user mentions the word "dalil" or "proof", they are referring to religious evidence from the Qur’an or the Sunnah, not from general documents or guides.
- If the answer cannot be found in the context, respond with: "The answer is not available in the provided context."
- Never provide any information that is not explicitly present in the context.
- Assume that the user may not be familiar with religious or technical terms — always explain clearly and simply when necessary.
- Be patient and empathetic — the user might be tired, confused, or under stress during their pilgrimage. Try to understand their true intent and respond accordingly.
- Your ultimate goal is to make the user feel supported, understood, and confident in performing their pilgrimage properly and with ease.
- Do not mention or refer to the context in your answer; respond as if you know the information directly.
- If the user asks about a specific action, do not just mention the name or title of the action — guide them step by step and include the full phrases or sayings if available, such as supplications or rituals.

</s>
<|user|>
Context:
{context}

Question:
{query}
</s>
<|assistant|>
""")


def generate_gemini_langchain(query, context_chunks):
    context = "\n\n".join(context_chunks)
    lang = detect_language(query)
    prompt = gemini_prompt_ar if lang == "ar" else gemini_prompt_en
    chain = (
        {"context": RunnablePassthrough(), "query": RunnablePassthrough()}
        | prompt
        | llm_gemini
        | StrOutputParser()
    )
    return clean_output(chain.invoke({"context": context, "query": query}))

# STEP 9: GPT-4

#Add your API Key here

llm_gpt4 = ChatOpenAI(model="gpt-4", temperature=0.7)

def generate_gpt4_langchain(query, context_chunks, lang="ar"):
    prompt = build_prompt(query, context_chunks, lang)
    return clean_output(llm_gpt4.invoke(prompt))

# STEP 10: ALLaM Model
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "ALLaM-AI/ALLaM-7B-Instruct-preview"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to("cpu")
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=200, device=-1)
llm_allam = HuggingFacePipeline(pipeline=pipe)

def build_allam_prompt(question, context_chunks):
    context = "\n\n".join(context_chunks)
    lang = detect_language(question)
    if lang == "ar":
        return f"""أنت مساعد ذكي ومؤتمن مخصص لخدمة الحجاج والمعتمرين، مهمتك هي تقديم إجابات دقيقة، وافية، ومفهومة لجميع أسئلتهم المتعلقة بالحج والعمرة، بما في ذلك المسائل الدينية، التنظيمية، الصحية، والعملية. يجب أن تكون إجاباتك شاملة، تراعي اختلاف مستويات فهم المستخدمين، وتتناسب مع احتياجاتهم أثناء أداء المناسك.

أجب فقط باستخدام المعلومات المتوفرة في السياق المعطى، ولا تعتمد على الحدس أو المعرفة العامة خارج هذا السياق. اقرأ وتحلّل جميع أجزاء السياق بدقة قبل الإجابة، ولا تكتفِ بأول جزء يحتوي على كلمات مشابهة للسؤال. وظّف جميع التفاصيل المتاحة لتقديم إجابة دقيقة وملائمة.

- إذا ورد في السؤال كلمة "دليل"، فالمقصود بها دليل شرعي من القرآن الكريم أو السنة النبوية الشريفة، وليس دليلاً تنظيمياً أو من وثائق عامة.
- إذا لم تجد إجابة واضحة في السياق، فأجب بعبارة: "الإجابة غير موجودة في السياق."
- لا تُدخل أي معلومات غير موجودة في السياق حتى وإن كنت تعتقد بصحتها.
- افترض أن المستخدم قد يكون غير ملم بالمصطلحات الشرعية أو التنظيمية، لذلك احرص على الشرح بلغة مبسطة وواضحة قدر الإمكان.
- كن صبورًا ومراعياً، فالمستخدم قد يكون في حالة تعب أو ارتباك أثناء أداء المناسك، وسؤاله قد لا يكون دقيقاً، لذا حاول فهم مقصده الحقيقي قبل الرد.
- الهدف هو أن يشعر المستخدم بأنك مساعد موثوق، يفهمه جيدًا، ويوفر له الدعم اللازم لأداء مناسكه بسهولة ويسر.
- لا تذكر السياق أو تشير إليه في إجابتك، أجب وكأنك تملك المعرفة بشكل مباشر.
- إذا سأل المستخدم عن فعل معين، فلا تكتفِ بذكر اسم الفعل أو عنوانه فقط، بل ارشده خطوة بخطوة، واذكر النصوص الكاملة إذا كانت متوفرة، مثل الأدعية أو الأقوال.


السياق:
{context}

السؤال:
{question}

الإجابة:"""
    else:
        return f"""
You are an intelligent and trustworthy assistant designed to support pilgrims during Hajj and Umrah. Your role is to provide accurate, comprehensive, and easy-to-understand answers to all user questions related to the pilgrimage, including religious, procedural, health-related, and logistical matters. Your responses must be clear, suitable for users from all backgrounds, and tailored to their needs during the performance of rituals.

Only answer using the information provided in the given context. Do not rely on general knowledge or assumptions. Carefully read and analyze the entire context before answering — do not rely solely on the first paragraph or the presence of keywords. Use all available details to deliver a precise and contextually relevant answer.

- If the user mentions the word "dalil" or "proof", they are referring to religious evidence from the Qur’an or the Sunnah, not from general documents or guides.
- If the answer cannot be found in the context, respond with: "The answer is not available in the provided context."
- Never provide any information that is not explicitly present in the context.
- Assume that the user may not be familiar with religious or technical terms — always explain clearly and simply when necessary.
- Be patient and empathetic — the user might be tired, confused, or under stress during their pilgrimage. Try to understand their true intent and respond accordingly.
- Your ultimate goal is to make the user feel supported, understood, and confident in performing their pilgrimage properly and with ease.
- Do not mention or refer to the context in your answer; respond as if you know the information directly.
- If the user asks about a specific action, do not just mention the name or title of the action — guide them step by step and include the full phrases or sayings if available, such as supplications or rituals.

Context:
{context}

Question:
{question}

Answer:"""


def generate_allam(prompt):
    full_output = llm_allam.invoke(prompt)
    # Split on the keyword that signals start of the actual answer
    if "الإجابة:" in full_output:
        return full_output.split("الإجابة:")[-1].strip()
    elif "Answer:" in full_output:
        return full_output.split("Answer:")[-1].strip()
    else:
        return full_output.strip()

"""# **RAGAs**"""

!pip install  openai weaviate-client ragas pypdf2

from datasets import Dataset
from PyPDF2 import PdfReader

from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)
from PyPDF2 import PdfReader
import google.generativeai as genai

json_file_path= "/content/GPT+Gemini_Testset.json"

import json

def extract_questions_and_answers_from_json(json_file_path):
    """
    Read questions and answers from a JSON file and format them for RAGAs evaluation.

    JSON should be a list of dictionaries with:
    - "question": the question string
    - "answer": the correct answer string

    Example:
    [
        {"question": "ما هو ركن الحج؟", "answer": "الوقوف بعرفة"},
        {"question": "كم عدد أيام الحج؟", "answer": "خمسة أيام"}
    ]
    """
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    questions = []
    ground_truths = []

    for item in data:
        question = item.get("question", "").strip()
        answer = item.get("answer", "").strip()

        if question and answer:
            questions.append(question)
            ground_truths.append([answer])

    print(f"✅ Extracted {len(questions)} question-answer pairs from JSON.")
    return questions, ground_truths


questions, ground_truths = extract_questions_and_answers_from_json(json_file_path)

import time
from google.api_core.exceptions import ResourceExhausted

answers = []
contexts = []
times = []


MAX_RETRIES = 3
INITIAL_RETRY_DELAY_SECONDS = 20
BACKOFF_FACTOR = 1.5


print(f"Starting processing with retry logic (Max Retries: {MAX_RETRIES}, Initial Delay: {INITIAL_RETRY_DELAY_SECONDS}s).")

for i, query in enumerate(questions):
    print(f"\nProcessing query {i+1}/{len(questions)}: '{query}'")

    retries = 0
    current_delay = INITIAL_RETRY_DELAY_SECONDS
    processed_successfully = False

    while retries < MAX_RETRIES and not processed_successfully:
        try:

            retrieved_docs = compression_retriever.invoke(query)
            context = [doc.page_content for doc in retrieved_docs]
            if retries == 0:
                 print(" -> Retrieval successful.")

            #  Generate answer
            print(f" -> Attempting generation (Attempt {retries + 1}/{MAX_RETRIES})...")
            start_time = time.time()
            answer = generate_gemini_langchain(query, context)
            end_time = time.time()
            print(" -> Generation successful.")

            generation_time = end_time - start_time
            times.append(generation_time)

            # Append results
            answers.append(answer)
            contexts.append(context)
            processed_successfully = True

        except ResourceExhausted as e:
            retries += 1
            print(f" !! Rate limit hit (Attempt {retries}/{MAX_RETRIES}). Error: {e}")
            if retries < MAX_RETRIES:
                print(f" -> Waiting for {current_delay:.1f} seconds before retrying...")
                time.sleep(current_delay)
                current_delay *= BACKOFF_FACTOR
            else:
                print(f" !! Max retries reached for query: '{query}'. Skipping.")

        except Exception as e:

            print(f" !! An unexpected error occurred (Attempt {retries + 1}): {e}")

            print(f" !! Skipping query '{query}' due to non-rate-limit error.")
            retries = MAX_RETRIES

    if not processed_successfully:
        print(f" -> Failed to process query '{query}' after all attempts.")


print("\nProcessing finished.")

# Create dataset for RAGAs evaluation
data = {
    "question": questions,
    "answer": answers,
    "contexts": contexts,
    "reference": ground_truths,
}

dataset = Dataset.from_dict(data)

print (data)

print("--- Inspecting ground_truths variable ---")
print(f"Type of ground_truths: {type(ground_truths)}")
if isinstance(ground_truths, list):
    print(f"Number of items in ground_truths: {len(ground_truths)}")
print("Full content of ground_truths:")
print(ground_truths)
print("--- End of inspection ---")

"""**Fix data for the required RAGAs structure**"""

flattened_ground_truths = [gt[0] for gt in ground_truths if gt and gt[0]]

data = {
    "question": questions,
    "answer": answers,
    "contexts": contexts,
    "reference": flattened_ground_truths,

dataset = Dataset.from_dict(data)

# Evaluate with RAGAs
import os

os.environ["OPENAI_API_KEY"] = "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"

result = evaluate(
    dataset=dataset,
    metrics=[context_precision, context_recall, faithfulness, answer_relevancy],
)

import pandas as pd
######## for each row###########
metric_columns = ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']

# Convert RAGAS result to DataFrame
df = result.to_pandas()

# drop long columns
df = df.drop(columns=["retrieved_contexts"], errors="ignore")
df['overall_quality'] = df[metric_columns].mean(axis=1)


pd.set_option("display.max_colwidth", None)
pd.set_option("display.expand_frame_repr", False)


display(df)

# overall score for the dataset
overall_avg_score = df['overall_quality'].mean()
print(f"Overall Average Score: {overall_avg_score}")

import pandas as pd
#################For each column##############


# Example columns from your output
metric_columns = ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']

# Calculate average for each metric
metric_averages = df[metric_columns].mean()

# Calculate overall average of the four metrics
overall_average = metric_averages.mean()


print("Average of each metric:")
print(metric_averages)
print("\nOverall average of the four metrics:", overall_average)

df.to_excel("ragas_evaluation_results_Gemeni.xlsx", index=False)

# Save and Download Results
df.to_json("ragas_evaluation_result_Gemeni.json", orient="records", force_ascii=False, indent=4)
with open('times.txt', 'w') as f:
    for item in times:
        f.write("%s\n" % item)

files.download("ragas_evaluation_result_Gemeni.json")
files.download("times.txt")

print("\n✅ All done. Files saved and downloaded successfully!")

# Compute average time
avg_time = sum(times) / len(times) if times else 0
print(f"\n⏱️ Average Time per Example: {avg_time:.3f} seconds")



"""# **Visualization of the results**"""

import pandas as pd
import matplotlib.pyplot as plt

file_path = "/content/ragas_evaluation_results_Gemeni.xlsx"
df = pd.read_excel(file_path)

metric_columns = ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']

# Drop rows with missing values in these columns
df_clean = df.dropna(subset=metric_columns)

# Compute average for each metric
average_metrics_computed = df_clean[metric_columns].mean()

# Compute overall average
overall_average_computed = average_metrics_computed.mean()


plt.figure(figsize=(6, 4))
bars = plt.bar(average_metrics_computed.index, average_metrics_computed.values, color='skyblue')

#Add value labels above each bar
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f"{height:.3f}",
             ha='center', va='bottom', fontsize=9)

plt.axhline(y=overall_average_computed, color='red', linestyle='--', label=f'Overall Avg: {overall_average_computed:.3f}')
plt.title("Average of RAGAS Evaluation Metrics for Gemeni")
plt.ylabel("Average Score")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt


file_path = "/content/ragas_evaluation_results_GPT4.xlsx"
df = pd.read_excel(file_path)


metric_columns = ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']

# Drop rows with missing values in these columns
df_clean = df.dropna(subset=metric_columns)

# Compute average for each metric
average_metrics_computed = df_clean[metric_columns].mean()

# Compute overall average
overall_average_computed = average_metrics_computed.mean()


plt.figure(figsize=(6, 4))
bars = plt.bar(average_metrics_computed.index, average_metrics_computed.values, color='skyblue')

# Add value labels above each bar
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f"{height:.3f}",
             ha='center', va='bottom', fontsize=9)

plt.axhline(y=overall_average_computed, color='red', linestyle='--', label=f'Overall Avg: {overall_average_computed:.3f}')
plt.title("Average of RAGAS Evaluation Metrics for GPT-4")
plt.ylabel("Average Score")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt


file_path = "/content/ragas_evaluation_result_ALLAM.xlsx"
df = pd.read_excel(file_path)


metric_columns = ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']

# Drop rows with missing values in these columns
df_clean = df.dropna(subset=metric_columns)

# Compute average for each metric
average_metrics_computed = df_clean[metric_columns].mean()

# Compute overall average
overall_average_computed = average_metrics_computed.mean()

plt.figure(figsize=(6, 4))
bars = plt.bar(average_metrics_computed.index, average_metrics_computed.values, color='skyblue')

#Add value labels above each bar
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f"{height:.3f}",
             ha='center', va='bottom', fontsize=9)


plt.axhline(y=overall_average_computed, color='red', linestyle='--', label=f'Overall Avg: {overall_average_computed:.3f}')
plt.title("Average of RAGAS Evaluation Metrics for ALLAM")
plt.ylabel("Average Score")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

files = {
    "GPT-4": "/content/ragas_evaluation_results_GPT4.xlsx",
    "Gemini": "/content/ragas_evaluation_results_Gemeni.xlsx",
    "ALLAM" : "/content/ragas_evaluation_result_ALLAM.xlsx"
}


metric_columns = ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']

# Store metric averages and overall quality
all_metrics = {}


for model_name, file_path in files.items():
    df = pd.read_excel(file_path)
    df_clean = df.dropna(subset=metric_columns)
    metric_avgs = df_clean[metric_columns].mean()
    metric_avgs['overall_quality'] = metric_avgs.mean()
    all_metrics[model_name] = metric_avgs


metrics_df = pd.DataFrame(all_metrics)


labels = metrics_df.index.tolist()
x = np.arange(len(labels))
width = 0.25

plt.figure(figsize=(10, 5))
bars = []

for i, model in enumerate(metrics_df.columns):
    bar = plt.bar(x + i * width, metrics_df[model], width, label=model)
    bars.append(bar)


for bar_group in bars:
    for bar in bar_group:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f"{height:.3f}",
                 ha='center', va='bottom', fontsize=9)


plt.xticks(x + width, labels, rotation=20)
plt.ylim(0, 1.05)
plt.ylabel("Average Score")
plt.title("RAGAS Metrics per LLM")
plt.legend()
plt.grid(axis='y')
plt.tight_layout()
plt.show()



import seaborn as sns

plt.figure(figsize=(8, 4))
sns.heatmap(metrics_df.T, annot=True, cmap="YlGnBu", vmin=0, vmax=1)
plt.title("RAGAS Metrics Heatmap per Model")
plt.xlabel("Metric")
plt.ylabel("LLM")
plt.tight_layout()
plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Simple Arabic/English language detection
def detect_language(text):
    for ch in str(text):
        if '\u0600' <= ch <= '\u06FF':
            return 'Arabic'
    return 'English'

files = {
    "GPT-4": "/content/ragas_evaluation_results_GPT4.xlsx",
    "Gemini": "/content/ragas_evaluation_results_Gemeni.xlsx",
    "ALLAM": "/content/ragas_evaluation_result_ALLAM.xlsx"
}

metric_columns = ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']
results = {}


for model_name, file_path in files.items():
    df = pd.read_excel(file_path)
    df['language'] = df['user_input'].apply(detect_language)
    df_clean = df.dropna(subset=metric_columns)

    # Group by language and compute metric means
    grouped = df_clean.groupby('language')[metric_columns].mean()
    grouped['overall_quality'] = grouped.mean(axis=1)

    for lang in grouped.index:
        results[(model_name, lang)] = grouped.loc[lang].tolist()


index = pd.MultiIndex.from_tuples(results.keys(), names=['Model', 'Language'])
metrics_df = pd.DataFrame(results.values(), index=index, columns=metric_columns + ['overall_quality'])

metrics = metric_columns + ['overall_quality']
languages = ['Arabic', 'English']
colors = ['#1f77b4', '#ff7f0e']

for model in metrics_df.index.get_level_values(0).unique():
    plt.figure(figsize=(6, 4))
    x = np.arange(len(metrics))
    bar_width = 0.35

    bars = []
    for i, lang in enumerate(languages):
        values = metrics_df.loc[(model, lang)][metrics]
        bar = plt.bar(x + i * bar_width, values, width=bar_width, label=lang, color=colors[i])
        bars.append(bar)


    for bar_group in bars:
        for bar in bar_group:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f"{height:.3f}",
                     ha='center', va='bottom', fontsize=8)

    plt.title(f"{model} - RAGAS Metrics by Language")
    plt.xticks(x + bar_width / 2, metrics, rotation=15)
    plt.ylim(0, 1.05)
    plt.ylabel("Average Score")
    plt.legend(title="Language")
    plt.grid(axis='y')
    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt
import numpy as np

#  Extract overall_quality per model/language
overall_quality_df = metrics_df['overall_quality'].unstack(level='Language')


models = overall_quality_df.index.tolist()
x = np.arange(len(models))
bar_width = 0.35
colors = ['#1f77b4', '#ff7f0e']

plt.figure(figsize=(8, 5))


bars_ar = plt.bar(x - bar_width/2, overall_quality_df['Arabic'], width=bar_width, label='Arabic', color=colors[0])
bars_en = plt.bar(x + bar_width/2, overall_quality_df['English'], width=bar_width, label='English', color=colors[1])

for bar in bars_ar + bars_en:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f"{height:.3f}",
             ha='center', va='bottom', fontsize=9)


plt.xticks(x, models)
plt.ylabel("Overall Quality Score")
plt.title("Overall RAGAS Quality Score by Language (Across Models)")
plt.ylim(0, 1.1)
plt.grid(axis='y')
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

files = {
    "GPT-4": "/content/TimesGPT4.txt",
    "Gemini": "/content/Gemeni Times.txt",
    "ALLAM": "/content/TimesALLAM.txt"
}

# Dictionary to store average response times
avg_times = {}

# Read and compute average time for each model
for model, path in files.items():
    with open(path, 'r') as f:
        times = [float(line.strip()) for line in f if line.strip()]
        avg = sum(times) / len(times) if times else 0
        avg_times[model] = avg


plt.figure(figsize=(7, 4))
bars = plt.bar(avg_times.keys(), avg_times.values(), color=['#1f77b4', '#ff7f0e', '#2ca02c'])


for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.1, f"{height:.2f}s",
             ha='center', va='bottom', fontsize=9)

plt.title("Average Response Time per Model")
plt.ylabel("Time (seconds)")
plt.ylim(0, max(avg_times.values()) * 1.2)
plt.grid(axis='y')
plt.tight_layout()
plt.show()